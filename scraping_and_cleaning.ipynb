{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping codde\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "\n",
    "service_name=\"youtube\"\n",
    "version=\"v3\"\n",
    "api_key=\"*****\"\n",
    "youtube=build(service_name,version,developerKey=api_key)\n",
    "\n",
    "video_details=[]\n",
    "playlist_ids=[]\n",
    "channel_ids=[\"UCIPPMRA040LQr5QPyJEbmXA\",#mr beast\n",
    "             \"UC85aYbNSFjsJdxfpxgQr8tA\",#judo slot\n",
    "             \"UC9YydG57epLqxA9cTzZXSeQ\",#cod\n",
    "]\n",
    "\n",
    "\n",
    "for channel_id in channel_ids:\n",
    "    try:\n",
    "        playlist_request = youtube.playlists().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            channelId=channel_id,\n",
    "            maxResults=100\n",
    "        )\n",
    "        playlist_response = playlist_request.execute()\n",
    "\n",
    "        for item in playlist_response['items']:\n",
    "            if 'id' in item:\n",
    "                playlist_ids.append(item['id'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(F\"Error:{e}\")\n",
    "print(f\"Playlist IDs: {playlist_ids}\")\n",
    "\n",
    "for playlist_id in playlist_ids:\n",
    "    next_page_token=None\n",
    "    video_count=0\n",
    "    while True:\n",
    "        try:\n",
    "            playlist_items_request = youtube.playlistItems().list(\n",
    "                    part=\"snippet,contentDetails\",\n",
    "                    maxResults=2,\n",
    "                    playlistId=playlist_id,\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            playlist_items_response = playlist_items_request.execute()\n",
    "            for item in playlist_items_response['items']:\n",
    "                if video_count < 2:\n",
    "                    video_id = item['contentDetails']['videoId']\n",
    "                    video_details_request = youtube.videos().list(\n",
    "                        part=\"snippet,contentDetails,statistics\",\n",
    "                        id=video_id\n",
    "                    )\n",
    "                    video_details_response = video_details_request.execute()\n",
    "                    video_details.append(video_details_response)\n",
    "                    video_count += 1\n",
    "\n",
    "            next_page_token = playlist_items_response.get('nextPageToken')\n",
    "            if not next_page_token or video_count >= 2:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching video details: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "with open('gaming.json', 'w') as file:\n",
    "    json.dump(video_details, file, indent=4)\n",
    "\n",
    "print(\"Video details have been saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " sports.json:\n",
    "    \"UC_WKb6N9iTGc77hxwXLDrbA\", # Sony sports Network \n",
    "    \"UCTl3QQTvqHFjurroKxexy2Q\", # Olympics  \n",
    "    \"UC14UlmYlSNiQCBe9Eookf_A\", # FC barcelona\n",
    "    \"UCWV3obpZVGgJ3j9FVhEjF2Q\", # Real madrid\n",
    "    \"UCmqfX0S3x0I3uwLkPdpX03w\" # Star sports english\n",
    "news.json:\n",
    "    \"UCYPvAwZP8pZhSMW8qs7cVCw\", #India today\n",
    "    \"UC16niRr50-MSBwiO3YDb3RA\" ,#BCC\n",
    "    \"UCZFMm1mMw0F81Z37aaEzTUA\", #ndtv\n",
    "    \"UCQXHTFnQYFTqN0MqmNh9EIA\", #bcc wwordls service\n",
    "    \"UCJEDFSxHHOW1PpBccdSxOTA\" #indian express  \n",
    "animals.json:\n",
    "    \"UCwmZiChSryoWQCZMIQezgTg\", # bcc earth\n",
    "    \"UCpVm7bg6pXKo1Pr6k5kxG9A\", #national geographic\n",
    "    \"UCbq-4OJxnziD3awH-aTezeA\",#real wild\n",
    "    \"UCQtW2oz8ec8pHjjxawujNjg\", #free documentary nature\n",
    "    \"UCz73YrQjemoqx6ZvVfJDAew\" #wildlife\n",
    "coding.json:\n",
    "    \"UCqrILQNl5Ed9Dz6CGMyvMTQ\",#clever programmer\n",
    "    \"UCWv7vMbMWH4-V0ZXdmDpPBA\",#progr with mosh\n",
    "    \"UC29ju8bIPH5as8OGnQzwJyA\",#trvaersy media\n",
    "    \"UCxX9wt5FWQUAAz4UrysqK9A\"#cs dojo\n",
    "food.json:\n",
    "    \"UCiAq_SU0ED1C6vWFMnw8Ekg\",# the food range\n",
    "    \"UCcAd5Np7fO8SeejB1FVKcYw\",#    best ever food review\n",
    "    \"UCIEv3lZ_tNXHzL3ox-_uUGQ\",#gordon ramsy\n",
    "    \"UCyEd6QBSgat5kkC6svyjudA\",#mark weins\n",
    "gaming.json:\n",
    "    \"UCIPPMRA040LQr5QPyJEbmXA\",#mr beast\n",
    "    \"UC85aYbNSFjsJdxfpxgQr8tA\",#judo slot\n",
    "    \"UC9YydG57epLqxA9cTzZXSeQ\",#cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the json files into s3 bucket\n",
    "\n",
    "import boto3\n",
    "access_key=\"\"\n",
    "secret_access_key=\"\"\n",
    "s3_client=boto3.client('s3',aws_access_key_id=access_key,aws_secret_access_key=secret_access_key)\n",
    "files=['sports.json','news.json','gaming.json','food.json','coding.json','animals.json']\n",
    "folder_name='files/'\n",
    "my_bucket='youtubedatafile1'\n",
    "for i in files:   \n",
    "    key_name=folder_name+i\n",
    "    try:\n",
    "        s3_client.upload_file(Filename=i,Bucket=my_bucket,Key=key_name)\n",
    "        print(\"inserted successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"no\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"sports.json\",\"r\") as file:\n",
    "    f1=json.load(file)\n",
    "sports=[]\n",
    "for i in f1:\n",
    "    if 'items' in i:\n",
    "        sports.extend(i['items'])\n",
    "sports_df=pd.json_normalize(sports)\n",
    "sports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news.json\",\"r\") as file:\n",
    "    f2=json.load(file)\n",
    "news=[]\n",
    "for i in f2:\n",
    "    if 'items' in i:\n",
    "        news.extend(i['items'])\n",
    "news_df=pd.json_normalize(news)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gaming.json\") as file:\n",
    "    f3=json.load(file)\n",
    "gaming=[]\n",
    "for i in f3:\n",
    "    if 'items' in i:\n",
    "        gaming.extend(i['items'])\n",
    "gaming_df=pd.json_normalize(gaming)\n",
    "gaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"food.json\",\"r\") as file:\n",
    "    f4=json.load(file)\n",
    "food=[]\n",
    "for i in f4:\n",
    "    if 'items' in i:\n",
    "        food.extend(i['items'])\n",
    "food_df=pd.json_normalize(food)\n",
    "food_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coding.json\",\"r\") as file:\n",
    "    f5=json.load(file)\n",
    "coding=[]\n",
    "for i in f5:\n",
    "    if 'items' in i:\n",
    "        coding.extend(i['items'])\n",
    "coding_df=pd.json_normalize(coding)\n",
    "coding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"animals.json\",\"r\") as file:\n",
    "    f5=json.load(file)\n",
    "animals=[]\n",
    "for i in f5:\n",
    "    if 'items' in i:\n",
    "        animals.extend(i['items'])\n",
    "animals_df=pd.json_normalize(animals)\n",
    "animals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([sports_df,news_df,gaming_df,food_df,coding_df,animals_df],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=['id','snippet.channelId','snippet.channelTitle','snippet.title','snippet.thumbnails.medium.url','snippet.tags'\n",
    "                  ,'statistics.viewCount','statistics.likeCount','statistics.commentCount','contentDetails.duration']\n",
    "df=df[selected_columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['Video_Id','Channel_Id','Channel_Title','Title','Url','Tags','View_Count','Like_Count','Comment_Count','Duration']\n",
    "df.to_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your PostgreSQL connection details\n",
    "username = 'postgres'\n",
    "password = '****'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "database = 'youtube_data'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'Data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the name of the table you want to insert data into\n",
    "table_name = 'youtube_data'\n",
    "\n",
    "# Push the data into PostgreSQL\n",
    "df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data has been successfully pushed to the PostgreSQL database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Tags','View_Count','Like_Count','Comment_Count','Duration']]=df[['Tags','View_Count','Like_Count','Comment_Count','Duration']].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='Video_Id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df=df\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your PostgreSQL connection details\n",
    "username = 'postgres'\n",
    "password = '****'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "database = 'youtube_data'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "table_name = 'cleaned_youtube_data'\n",
    "\n",
    "# Push the data into PostgreSQL\n",
    "cleaned_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data has been successfully pushed to the PostgreSQL database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.loc[:, 'Tags'] = cleaned_df['Tags'].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\",\", \"\").replace(\"-\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace('\"',\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"Tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rows in cleaned_df['Tags']:\n",
    "    print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Assuming 'cleaned_df' is defined with a 'Tags' column\n",
    "vectorizer = TfidfVectorizer(max_features=500, lowercase=True, stop_words=\"english\", ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(cleaned_df['Tags'])\n",
    "\n",
    "# Apply PCA to reduce the dimensionality of the TF-IDF matrix to 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Set the number of clusters (you can change this to any number you want to evaluate)\n",
    "no_clusters = 100\n",
    "kmeans = KMeans(n_clusters=no_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "cleaned_df.loc[:, 'Cluster'] = cluster_labels\n",
    "\n",
    "\n",
    "# Calculate the Silhouette Score for the given number of clusters\n",
    "silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "print(f\"Silhouette Score for {no_clusters} clusters: {silhouette_avg}\")\n",
    "\n",
    "# Scatter plot of the PCA-reduced data colored by cluster labels\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6, edgecolor='k')\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title(f'Scatter Plot of Clusters (KMeans with {no_clusters} clusters)')\n",
    "plt.show()\n",
    "\n",
    "with open(\"vectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(vectorizer,f)\n",
    "\n",
    "with open(\"pca.pkl\",\"wb\") as f:\n",
    "    pickle.dump(pca,f)\n",
    "\n",
    "with open(\"kmeans.pkl\",\"wb\") as f:\n",
    "    pickle.dump(kmeans,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Define your PostgreSQL connection details\n",
    "username = 'postgres'\n",
    "password = '****'\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "database = 'youtube_data'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "table_name = 'final_youtube_data'\n",
    "\n",
    "# Push the data into PostgreSQL\n",
    "cleaned_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data has been successfully pushed to the PostgreSQL database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
